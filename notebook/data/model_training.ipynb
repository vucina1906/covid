{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USMER</th>\n",
       "      <th>MEDICAL_UNIT</th>\n",
       "      <th>SEX</th>\n",
       "      <th>HOSPITALIZED</th>\n",
       "      <th>DIED</th>\n",
       "      <th>PNEUMONIA</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PREGNANT</th>\n",
       "      <th>DIABETES</th>\n",
       "      <th>COPD</th>\n",
       "      <th>ASTHMA</th>\n",
       "      <th>INMSUPR</th>\n",
       "      <th>HIPERTENSION</th>\n",
       "      <th>OTHER_DISEASE</th>\n",
       "      <th>CARDIOVASCULAR</th>\n",
       "      <th>OBESITY</th>\n",
       "      <th>RENAL_CHRONIC</th>\n",
       "      <th>TOBACCO</th>\n",
       "      <th>ANTIGEN_TEST</th>\n",
       "      <th>Age_Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>60-69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>70-79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>50-59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>50-59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>60-69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   USMER  MEDICAL_UNIT     SEX  HOSPITALIZED  DIED  PNEUMONIA  AGE  PREGNANT   \n",
       "0      2             1  Female             0     1        1.0   65       0.0  \\\n",
       "1      2             1    Male             0     1        1.0   72       0.0   \n",
       "2      2             1    Male             1     1        0.0   55       0.0   \n",
       "3      2             1  Female             0     1        0.0   53       0.0   \n",
       "4      2             1    Male             0     1        0.0   68       0.0   \n",
       "\n",
       "   DIABETES  COPD  ASTHMA  INMSUPR  HIPERTENSION  OTHER_DISEASE   \n",
       "0       0.0   0.0     0.0      0.0           1.0            0.0  \\\n",
       "1       0.0   0.0     0.0      0.0           1.0            0.0   \n",
       "2       1.0   0.0     0.0      0.0           0.0            0.0   \n",
       "3       0.0   0.0     0.0      0.0           0.0            0.0   \n",
       "4       1.0   0.0     0.0      0.0           1.0            0.0   \n",
       "\n",
       "   CARDIOVASCULAR  OBESITY  RENAL_CHRONIC  TOBACCO  ANTIGEN_TEST Age_Group  \n",
       "0             0.0      0.0            0.0      0.0             1     60-69  \n",
       "1             0.0      1.0            1.0      0.0             0     70-79  \n",
       "2             0.0      0.0            0.0      0.0             1     50-59  \n",
       "3             0.0      0.0            0.0      0.0             0     50-59  \n",
       "4             0.0      0.0            0.0      0.0             1     60-69  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('df_EDA.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not able to train Grid Search CV with parameters hypertuning on whole dataset so take a sample\n",
    "df_sampled = df.sample(n=50000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_sampled.drop(columns=['DIED','Age_Group','MEDICAL_UNIT'],axis=1)\n",
    "y = df_sampled[\"DIED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X.select_dtypes(exclude=\"object\").columns\n",
    "cat_features = X.select_dtypes(include=\"object\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_feat_transformer = StandardScaler()\n",
    "cat_feat_transformer = OneHotEncoder()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"OneHotEncoder\", cat_feat_transformer, cat_features),\n",
    "         (\"StandardScaler\", num_feat_transformer, num_features),        \n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 18), (10000, 18))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/40\n",
      "Processing batch 2/40\n",
      "Processing batch 3/40\n",
      "Processing batch 4/40\n",
      "Processing batch 5/40\n",
      "Processing batch 6/40\n",
      "Processing batch 7/40\n",
      "Processing batch 8/40\n",
      "Processing batch 9/40\n",
      "Processing batch 10/40\n",
      "Processing batch 11/40\n",
      "Processing batch 12/40\n",
      "Processing batch 13/40\n",
      "Processing batch 14/40\n",
      "Processing batch 15/40\n",
      "Processing batch 16/40\n",
      "Processing batch 17/40\n",
      "Processing batch 18/40\n",
      "Processing batch 19/40\n",
      "Processing batch 20/40\n",
      "Processing batch 21/40\n",
      "Processing batch 22/40\n",
      "Processing batch 23/40\n",
      "Processing batch 24/40\n",
      "Processing batch 25/40\n",
      "Processing batch 26/40\n",
      "Processing batch 27/40\n",
      "Processing batch 28/40\n",
      "Processing batch 29/40\n",
      "Processing batch 30/40\n",
      "Processing batch 31/40\n",
      "Processing batch 32/40\n",
      "Processing batch 33/40\n",
      "Processing batch 34/40\n",
      "Processing batch 35/40\n",
      "Processing batch 36/40\n",
      "Processing batch 37/40\n",
      "Processing batch 38/40\n",
      "Processing batch 39/40\n",
      "Processing batch 40/40\n",
      "Best parameters: {'rf__max_depth': 5, 'rf__n_estimators': 200}\n",
      "Best score: 0.9219999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import parallel_backend\n",
    "import numpy as np\n",
    "\n",
    "# Define pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [50, 100, 200],\n",
    "    'rf__max_depth': [5, 10, 20]\n",
    "}\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# Split data into batches\n",
    "X_batches = np.array_split(X_train, np.ceil(len(X_train)/batch_size))\n",
    "y_batches = np.array_split(y_train, np.ceil(len(y_train)/batch_size))\n",
    "\n",
    "# Perform grid search with parallel processing\n",
    "with parallel_backend('multiprocessing'):\n",
    "    for i, (X_batch, y_batch) in enumerate(zip(X_batches, y_batches)):\n",
    "        print(f\"Processing batch {i+1}/{len(X_batches)}\")\n",
    "        if len(X_batch) < 2: # check if batch size is greater than 1\n",
    "            continue\n",
    "        grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "        grid.fit(X_batch, y_batch)\n",
    "\n",
    "# Print overall best parameters and score\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Best score: {grid.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9395\n"
     ]
    }
   ],
   "source": [
    "# Define pipeline with best hyperparameters\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=5))\n",
    "])\n",
    "\n",
    "# Fit pipeline on entire training set\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Use pipeline to predict on new data\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_rf}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With using Random Forest Classifier we get accuracy 94%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/40\n",
      "Processing batch 2/40\n",
      "Processing batch 3/40\n",
      "Processing batch 4/40\n",
      "Processing batch 5/40\n",
      "Processing batch 6/40\n",
      "Processing batch 7/40\n",
      "Processing batch 8/40\n",
      "Processing batch 9/40\n",
      "Processing batch 10/40\n",
      "Processing batch 11/40\n",
      "Processing batch 12/40\n",
      "Processing batch 13/40\n",
      "Processing batch 14/40\n",
      "Processing batch 15/40\n",
      "Processing batch 16/40\n",
      "Processing batch 17/40\n",
      "Processing batch 18/40\n",
      "Processing batch 19/40\n",
      "Processing batch 20/40\n",
      "Processing batch 21/40\n",
      "Processing batch 22/40\n",
      "Processing batch 23/40\n",
      "Processing batch 24/40\n",
      "Processing batch 25/40\n",
      "Processing batch 26/40\n",
      "Processing batch 27/40\n",
      "Processing batch 28/40\n",
      "Processing batch 29/40\n",
      "Processing batch 30/40\n",
      "Processing batch 31/40\n",
      "Processing batch 32/40\n",
      "Processing batch 33/40\n",
      "Processing batch 34/40\n",
      "Processing batch 35/40\n",
      "Processing batch 36/40\n",
      "Processing batch 37/40\n",
      "Processing batch 38/40\n",
      "Processing batch 39/40\n",
      "Processing batch 40/40\n",
      "Best parameters: {'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
      "Best score: 0.9260000000000002\n"
     ]
    }
   ],
   "source": [
    "#Using grid search for XGBC algoritham.\n",
    "\n",
    "# Define pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', XGBClassifier())\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [50, 100, 200],\n",
    "    'xgb__max_depth': [5, 10, 20],\n",
    "    'xgb__learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# Split data into batches\n",
    "X_batches = np.array_split(X_train, np.ceil(len(X_train)/batch_size))\n",
    "y_batches = np.array_split(y_train, np.ceil(len(y_train)/batch_size))\n",
    "\n",
    "# Perform grid search with parallel processing\n",
    "with parallel_backend('multiprocessing'):\n",
    "    for i, (X_batch, y_batch) in enumerate(zip(X_batches, y_batches)):\n",
    "        print(f\"Processing batch {i+1}/{len(X_batches)}\")\n",
    "        if len(X_batch) < 2: # check if batch size is greater than 1\n",
    "            continue\n",
    "        grid = GridSearchCV(pipe, param_grid, cv=5,n_jobs=-1)\n",
    "        grid.fit(X_batch, y_batch)\n",
    "\n",
    "# Print overall best parameters and score\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Best score: {grid.best_score_}\")\n",
    "\n",
    "#Best parameters: {'xgb__learning_rate': 0.01, 'xgb__max_depth': 5, 'xgb__n_estimators': 50}\n",
    "#Best score: 0.9260000000000002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9404\n"
     ]
    }
   ],
   "source": [
    "# Define pipeline with best hyperparameters\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', XGBClassifier(learning_rate=0.01, max_depth=5, n_estimators=50))\n",
    "])\n",
    "\n",
    "# Fit pipeline on entire training set\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Use pipeline to predict on new data\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_xgb}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With XGBOOST we get Accuracy 94%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/40\n",
      "Processing batch 2/40\n",
      "Processing batch 3/40\n",
      "Processing batch 4/40\n",
      "Processing batch 5/40\n",
      "Processing batch 6/40\n",
      "Processing batch 7/40\n",
      "Processing batch 8/40\n",
      "Processing batch 9/40\n",
      "Processing batch 10/40\n",
      "Processing batch 11/40\n",
      "Processing batch 12/40\n",
      "Processing batch 13/40\n",
      "Processing batch 14/40\n",
      "Processing batch 15/40\n",
      "Processing batch 16/40\n",
      "Processing batch 17/40\n",
      "Processing batch 18/40\n",
      "Processing batch 19/40\n",
      "Processing batch 20/40\n",
      "Processing batch 21/40\n",
      "Processing batch 22/40\n",
      "Processing batch 23/40\n",
      "Processing batch 24/40\n",
      "Processing batch 25/40\n",
      "Processing batch 26/40\n",
      "Processing batch 27/40\n",
      "Processing batch 28/40\n",
      "Processing batch 29/40\n",
      "Processing batch 30/40\n",
      "Processing batch 31/40\n",
      "Processing batch 32/40\n",
      "Processing batch 33/40\n",
      "Processing batch 34/40\n",
      "Processing batch 35/40\n",
      "Processing batch 36/40\n",
      "Processing batch 37/40\n",
      "Processing batch 38/40\n",
      "Processing batch 39/40\n",
      "Processing batch 40/40\n",
      "Best parameters: {'lgbm__learning_rate': 0.01, 'lgbm__max_depth': 5, 'lgbm__n_estimators': 200}\n",
      "Best score: 0.9259999999999999\n"
     ]
    }
   ],
   "source": [
    "#Using grid search for LGBMC algoritham.\n",
    "\n",
    "# Define pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lgbm', LGBMClassifier())\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'lgbm__n_estimators': [50, 100, 200],\n",
    "    'lgbm__max_depth': [5, 10, 20],\n",
    "    'lgbm__learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# Split data into batches\n",
    "X_batches = np.array_split(X_train, np.ceil(len(X_train)/batch_size))\n",
    "y_batches = np.array_split(y_train, np.ceil(len(y_train)/batch_size))\n",
    "\n",
    "# Perform grid search with parallel processing\n",
    "with parallel_backend('multiprocessing'):\n",
    "    for i, (X_batch, y_batch) in enumerate(zip(X_batches, y_batches)):\n",
    "        print(f\"Processing batch {i+1}/{len(X_batches)}\")\n",
    "        if len(X_batch) < 2: # check if batch size is greater than 1\n",
    "            continue\n",
    "        grid = GridSearchCV(pipe, param_grid, cv=5,n_jobs=-1)\n",
    "        grid.fit(X_batch, y_batch)\n",
    "\n",
    "# Print overall best parameters and score\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Best score: {grid.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9417\n"
     ]
    }
   ],
   "source": [
    "# Define pipeline with best hyperparameters\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', LGBMClassifier(learning_rate=0.01, max_depth=5, n_estimators=200))\n",
    "])\n",
    "\n",
    "# Fit pipeline on entire training set\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Use pipeline to predict on new data\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_lgbm = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_lgbm}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Again with LGBM we have accuracy 94%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/20\n",
      "Processing batch 2/20\n",
      "Processing batch 3/20\n",
      "Processing batch 4/20\n",
      "Processing batch 5/20\n",
      "Processing batch 6/20\n",
      "Processing batch 7/20\n",
      "Processing batch 8/20\n",
      "Processing batch 9/20\n",
      "Processing batch 10/20\n",
      "Processing batch 11/20\n",
      "Processing batch 12/20\n",
      "Processing batch 13/20\n",
      "Processing batch 14/20\n",
      "Processing batch 15/20\n",
      "Processing batch 16/20\n",
      "Processing batch 17/20\n",
      "Processing batch 18/20\n",
      "Processing batch 19/20\n",
      "Processing batch 20/20\n",
      "Best parameters: {'catboost__depth': 5, 'catboost__learning_rate': 0.1, 'catboost__n_estimators': 50}\n",
      "Best score: 0.9380000000000001\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import parallel_backend\n",
    "import numpy as np\n",
    "\n",
    "# Define pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('catboost', CatBoostClassifier(verbose=False))\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'catboost__n_estimators': [50, 100, 200],\n",
    "    'catboost__depth': [5, 10],\n",
    "    'catboost__learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 2000\n",
    "\n",
    "# Split data into batches\n",
    "X_batches = np.array_split(X_train, np.ceil(len(X_train)/batch_size))\n",
    "y_batches = np.array_split(y_train, np.ceil(len(y_train)/batch_size))\n",
    "\n",
    "# Perform grid search with parallel processing\n",
    "with parallel_backend('multiprocessing'):\n",
    "    for i, (X_batch, y_batch) in enumerate(zip(X_batches, y_batches)):\n",
    "        print(f\"Processing batch {i+1}/{len(X_batches)}\")\n",
    "        if len(X_batch) < 2: # check if batch size is greater than 1\n",
    "            continue\n",
    "        grid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1)\n",
    "        grid.fit(X_batch, y_batch)\n",
    "\n",
    "# Print overall best parameters and score\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Best score: {grid.best_score_}\")\n",
    "#We get \n",
    "#Best parameters: {'catboost__depth': 5, 'catboost__learning_rate': 0.1, 'catboost__n_estimators': 50}\n",
    "#Best score: 0.9380000000000001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6158865\ttotal: 4.94ms\tremaining: 242ms\n",
      "1:\tlearn: 0.5522485\ttotal: 12.7ms\tremaining: 304ms\n",
      "2:\tlearn: 0.4982570\ttotal: 18.4ms\tremaining: 288ms\n",
      "3:\tlearn: 0.4534917\ttotal: 22.5ms\tremaining: 259ms\n",
      "4:\tlearn: 0.4145703\ttotal: 30.1ms\tremaining: 271ms\n",
      "5:\tlearn: 0.3814742\ttotal: 34.4ms\tremaining: 252ms\n",
      "6:\tlearn: 0.3522454\ttotal: 38.6ms\tremaining: 237ms\n",
      "7:\tlearn: 0.3268993\ttotal: 45.6ms\tremaining: 239ms\n",
      "8:\tlearn: 0.3048862\ttotal: 50ms\tremaining: 228ms\n",
      "9:\tlearn: 0.2852965\ttotal: 54.1ms\tremaining: 217ms\n",
      "10:\tlearn: 0.2681207\ttotal: 61ms\tremaining: 216ms\n",
      "11:\tlearn: 0.2532563\ttotal: 65.8ms\tremaining: 208ms\n",
      "12:\tlearn: 0.2397529\ttotal: 69.9ms\tremaining: 199ms\n",
      "13:\tlearn: 0.2280140\ttotal: 76.4ms\tremaining: 196ms\n",
      "14:\tlearn: 0.2174669\ttotal: 80.8ms\tremaining: 189ms\n",
      "15:\tlearn: 0.2080338\ttotal: 84.9ms\tremaining: 180ms\n",
      "16:\tlearn: 0.2002321\ttotal: 88.5ms\tremaining: 172ms\n",
      "17:\tlearn: 0.1929531\ttotal: 94.7ms\tremaining: 168ms\n",
      "18:\tlearn: 0.1866900\ttotal: 98.9ms\tremaining: 161ms\n",
      "19:\tlearn: 0.1806787\ttotal: 103ms\tremaining: 155ms\n",
      "20:\tlearn: 0.1754716\ttotal: 110ms\tremaining: 152ms\n",
      "21:\tlearn: 0.1705715\ttotal: 115ms\tremaining: 146ms\n",
      "22:\tlearn: 0.1666537\ttotal: 119ms\tremaining: 140ms\n",
      "23:\tlearn: 0.1628686\ttotal: 127ms\tremaining: 137ms\n",
      "24:\tlearn: 0.1592798\ttotal: 131ms\tremaining: 131ms\n",
      "25:\tlearn: 0.1561229\ttotal: 136ms\tremaining: 125ms\n",
      "26:\tlearn: 0.1534122\ttotal: 144ms\tremaining: 122ms\n",
      "27:\tlearn: 0.1509555\ttotal: 148ms\tremaining: 116ms\n",
      "28:\tlearn: 0.1485332\ttotal: 152ms\tremaining: 110ms\n",
      "29:\tlearn: 0.1467464\ttotal: 159ms\tremaining: 106ms\n",
      "30:\tlearn: 0.1447761\ttotal: 164ms\tremaining: 101ms\n",
      "31:\tlearn: 0.1429787\ttotal: 168ms\tremaining: 94.7ms\n",
      "32:\tlearn: 0.1415847\ttotal: 175ms\tremaining: 90.3ms\n",
      "33:\tlearn: 0.1405151\ttotal: 180ms\tremaining: 84.6ms\n",
      "34:\tlearn: 0.1391949\ttotal: 184ms\tremaining: 78.8ms\n",
      "35:\tlearn: 0.1380341\ttotal: 191ms\tremaining: 74.1ms\n",
      "36:\tlearn: 0.1373994\ttotal: 195ms\tremaining: 68.4ms\n",
      "37:\tlearn: 0.1363997\ttotal: 199ms\tremaining: 62.8ms\n",
      "38:\tlearn: 0.1356233\ttotal: 207ms\tremaining: 58.3ms\n",
      "39:\tlearn: 0.1351559\ttotal: 211ms\tremaining: 52.7ms\n",
      "40:\tlearn: 0.1348513\ttotal: 215ms\tremaining: 47.1ms\n",
      "41:\tlearn: 0.1341209\ttotal: 222ms\tremaining: 42.2ms\n",
      "42:\tlearn: 0.1334173\ttotal: 228ms\tremaining: 37ms\n",
      "43:\tlearn: 0.1328622\ttotal: 238ms\tremaining: 32.4ms\n",
      "44:\tlearn: 0.1323512\ttotal: 244ms\tremaining: 27.1ms\n",
      "45:\tlearn: 0.1321929\ttotal: 249ms\tremaining: 21.6ms\n",
      "46:\tlearn: 0.1319430\ttotal: 259ms\tremaining: 16.5ms\n",
      "47:\tlearn: 0.1314470\ttotal: 263ms\tremaining: 11ms\n",
      "48:\tlearn: 0.1309590\ttotal: 270ms\tremaining: 5.5ms\n",
      "49:\tlearn: 0.1305411\ttotal: 275ms\tremaining: 0us\n",
      "Accuracy: 0.9416\n"
     ]
    }
   ],
   "source": [
    "# Define pipeline with best hyperparameters\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', CatBoostClassifier(learning_rate=0.1, depth=5, n_estimators=50))\n",
    "])\n",
    "\n",
    "# Fit pipeline on entire training set\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Use pipeline to predict on new data\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_cb = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_cb}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Also with catboost Accuracy is 94%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/20\n",
      "Processing batch 2/20\n",
      "Processing batch 3/20\n",
      "Processing batch 4/20\n",
      "Processing batch 5/20\n",
      "Processing batch 6/20\n",
      "Processing batch 7/20\n",
      "Processing batch 8/20\n",
      "Processing batch 9/20\n",
      "Processing batch 10/20\n",
      "Processing batch 11/20\n",
      "Processing batch 12/20\n",
      "Processing batch 13/20\n",
      "Processing batch 14/20\n",
      "Processing batch 15/20\n",
      "Processing batch 16/20\n",
      "Processing batch 17/20\n",
      "Processing batch 18/20\n",
      "Processing batch 19/20\n",
      "Processing batch 20/20\n",
      "Best parameters: {'dt__criterion': 'gini', 'dt__max_depth': 5, 'dt__min_samples_leaf': 2, 'dt__min_samples_split': 5}\n",
      "Best score: 0.932\n"
     ]
    }
   ],
   "source": [
    "# Define pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'dt__max_depth': [5, 10, 15],\n",
    "    'dt__min_samples_split': [2, 5, 10],\n",
    "    'dt__min_samples_leaf': [1, 2, 4],\n",
    "    'dt__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 2000\n",
    "\n",
    "# Split data into batches\n",
    "X_batches = np.array_split(X_train, np.ceil(len(X_train)/batch_size))\n",
    "y_batches = np.array_split(y_train, np.ceil(len(y_train)/batch_size))\n",
    "\n",
    "# Perform grid search with parallel processing\n",
    "with parallel_backend('multiprocessing'):\n",
    "    for i, (X_batch, y_batch) in enumerate(zip(X_batches, y_batches)):\n",
    "        print(f\"Processing batch {i+1}/{len(X_batches)}\")\n",
    "        if len(X_batch) < 2: # check if batch size is greater than 1\n",
    "            continue\n",
    "        grid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1)\n",
    "        grid.fit(X_batch, y_batch)\n",
    "\n",
    "# Print overall best parameters and score\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Best score: {grid.best_score_}\")\n",
    "#We get \n",
    "#Best parameters: {'dt__criterion': 'gini', 'dt__max_depth': 5, 'dt__min_samples_leaf': 2, 'dt__min_samples_split': 5}\n",
    "#Best score: 0.932"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9394\n"
     ]
    }
   ],
   "source": [
    "# Define pipeline with best hyperparameters\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', RandomForestClassifier(criterion='gini', max_depth=5, min_samples_leaf=2,min_samples_split=5))\n",
    "])\n",
    "\n",
    "# Fit pipeline on entire training set\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Use pipeline to predict on new data\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_dt = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_dt}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regarding that we get best accuracy with LGBM we will train that model with best parameters and save it as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lgbm_model.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# Define pipeline with best parameters\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lgbm', LGBMClassifier(n_estimators=200, max_depth=5, learning_rate=0.01))\n",
    "])\n",
    "\n",
    "# Train model on entire training set\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Save trained model to disk\n",
    "joblib.dump(pipe, 'lgbm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
